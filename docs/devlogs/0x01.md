# Higancc Devlog 0x01

I'm interested in compilers. I'm always fascinated by how we can tell your computers what you want to do by simply writing commands like what we do in human languages because computers only understand machine language. So I decided to get into the compilers rabbit hole to understand one of the greatest dark magics in the world of computer science and engineering. 

## Motivation
(If you don't care about my story, feel free to jump to the next section, I won't be mad)

The very first instinct for me was hitting up the Google "how can I make a compiler?", and somehow it suggested me the notorious [Compilers: principles, techniques, and tools](https://www.amazon.com.au/Compilers-Alfred-V-Aho/dp/0321486811) or widely known as the "Dragon Book". 

> Insert Dragon Book cover pic here

Turns out, I can't digest a single lesson from this book because it is ways too theoretical for me, and I was a complete novice at Data Structures and Algorithm. So I kept searching for down-to-earth resources that I can get my hand dirty. Fortunately, I came across the [Crafting Interpreters](https://craftinginterpreters.com/) by Robert Nystrom. This book is literal gold mine, I finally understand how a language interpreter works by generating an Abstract Syntax Tree (AST) from the source code in plain-text form, then traversing the AST to perform actions known as executing statements.

> Insert Crafting Interpreters cover pic and AST here

Furthermore, that book also covers on how to write a bytecode interpreter and a bytecode virtual machine to enhance the performance of the interpreter and I eventually understand how most well-known interpreters like Java, Python, Ruby, .etc works.

However, as a mechatronics undergrad, I am more fascinated at low-level languages like C, C++, Rust and Zig and how their compilers are much faster than what I made from the earlier said book. Turns out their compilers translate down straight to machine code (or assembly language) instead of incorporating a bytecode interpreter to reduce runtime overhead. Therefore, I took the next step of learning how to make a real-deal compiler. Luckily, in this year, [Nora Sandler](https://norasandler.com/about/) published her [Writing a C Compiler](https://nostarch.com/writing-c-compiler) book in 2022. I can't be happier, then I borrowed the book from my friend to continue on my compiler's journey. Before we start, each devlogs are simply my writeups for each chapters of this book. 

## A Minimal Compiler
### Lexical Analysis (Lexer)
For those who doesn't have any idea about a lexer, it basically transform a source file into a list of tokens. Taking an example c program:
```c
int main(void) {
	return 0;
}
```

When putting this source file into the lexer, it returns a list of token as follows:
```
KEYWORD_INT
IDENTIFIER ("main")
LEFT_PAREN
KEYWORD_VOID
RIGHT_PAREN
LEFT_BRACE
KEYWORD_RETURN
INT ("0")
SEMICOLON
RIGHT_BRACE
```

In this token list, there are two tokens that have extra information like `IDENTIFER` and `INT` tokens meanwhile others don't. From the Nora Sandler's book, An identifier is an ASCII letter or underscore followed by a mix of letters, underscores, and digits and they are case sensitive, whereas an integer constant consists of one or more digits. In this stage, the token can be classified from this table:
| Token | Rule |
|--|--|
| Identifier | Alphabet characters and/or '_' |
|Int constant|Digits (0-9)|
|Float constant|Digits (0-9) and '.' in between|
|int keyword|"int"|
|float keyword|"float"|
|void keyword|"void"|
|return keyword|"return"|
|Left parenthesis|"("|
|Right parenthesis|")"|
|Left brace|"{"|
|Right brace|"}"|
|Semicolon|";"|



The working flow of the lexer can be expressed like:
```
scanTokens(src_file) -> Tokens {
	tokens = []
	while src_file is not at end {
		token = scanToken()
		tokens.push(token)
	}
	return tokens
}

scanToken() -> Token {
	skip any whitespaces
	if there is any matches from the rules of the table {
		create a token object with its respective token type and matched substring
	}
	else {
		return an error token and its message
	}
}
```

Now we have the token table defined along with the pseudocode, let's define the Token structure:
```c
typedef struct {
	TokenType type;
	const char *start;
	int length;
	int line;
} Token;
```
In this structure, `start` points to the beginning of the substring that matches the token rule, and `length` determine how long the substring is. You don't have to know about `line` for now, I just put it for better debugging experience.

Moving on, here's the Lexer struct:
```c
typedef struct {
  const char *start;
  const char *current;
  int line;
} Lexer;
```
`start` is similar to Token's `start`, but start pointer continues to work with sequencing characters until the end of file, and `current` serves as a helper pointer to keep track of forming substrings compares to the `start` pointer. Again, the `line` member solely for debugging purpose. Let's move on to the core logic of the lexer.

```c
Token scanToken(Lexer *lexer) {
	skip_whitespace(lexer);
  lexer->start = lexer->current;
	
	// return end-of-file token 
  if (is_at_end(lexer)) return make_token(lexer, TOKEN_EOF);
	
  char c = advance(lexer);
  if (is_alpha(c)) return identifier(lexer);
  if (is_digit(c)) return number(lexer);

  switch (c) {
    case '(': return make_token(lexer, TOKEN_LEFT_PAREN);
    case ')': return make_token(lexer, TOKEN_RIGHT_PAREN);
    case '{': return make_token(lexer, TOKEN_LEFT_BRACE);
    case '}': return make_token(lexer, TOKEN_RIGHT_BRACE);
    case ';': return make_token(lexer, TOKEN_SEMICOLON);
  }
  return error_token(lexer, "Unexpected character.");
}
```

This is the reduced code from the actual lexer code to make it easier for you to follow. Here are some lexer's helper functions that you need to know to understand this logic:

- skip_whitespace: You probably guessed it, skip through all `\r`, `\t` and `' '`. Keep in mind that you need to increment line number on `\n` character
- make_token: returns a Token object with specified token type. I'll explain error_token later
- peek: return the current character
- peek_next: return the next of the current character. For example: if current points to "abc...", then peek_next returns 'b'
- advance: same as peek but also point to the next character


Now you know how to capture single character tokens like parentheses and braces, let's see how the lexer captures identifier and integer constants.

```c
static Token identifier(Lexer *lexer) {
  while (is_alpha(peek(lexer)) || is_digit(peek(lexer))) advance(lexer);
  return make_token(lexer, TOKEN_IDENTIFIER);
}

static Token number(Lexer *lexer) {
  if (is_alpha(peek_next(lexer))) {
    while (is_alpha(peek(lexer)) || is_digit(peek(lexer))) advance(lexer); 
    return error_token(lexer, "Identifier must not contain any number at the beginning");
  }
  while (is_digit(peek(lexer))) advance(lexer);

  if (peek(lexer) == '.' && is_digit(peek_next(lexer))) {
    advance(lexer);

    while (is_digit(peek(lexer))) advance(lexer);
    return make_token(lexer, TOKEN_FLOAT);
  }
  
  return make_token(lexer, TOKEN_INT);
}
```

Identifiers can be detected by checking if the current character is an alphabet character, then we keep moving until we found non-alphanumeric character, then create an identifier token. As for constant token, the lexer keep advancing until there is no digit character then it returns an integer constant token. In addition, it returns a float constant if it detects a "." character within digits.

Let's add a token print function to dump out token information to test the lexer:
```c
void print_token(const Token *token) {
  printf("%4d ", token->line); 
  printf("%s \"%.*s\"\n", token_type_strings[token->type], token->length, token->start);
}
```

Shall we lexing some code!
![return_0.c lexing output](https://i.imgur.com/uf9ZRer.png)
![enter image description here](https://i.imgur.com/trOKxjN.png)
![enter image description here](https://i.imgur.com/OFFyza9.png)
Having the lexer worked, let's go to parsing!

### Parser
Parsing is the crucial next step after lexical analysis in the compilation process. It takes the flat list of tokens generated by the lexer and constructs a hierarchical representation of the program's structure, typically in the form of an Abstract Syntax Tree (AST). The parser analyzes the sequence of tokens according to the grammar rules of the C language, grouping them into meaningful syntactic units such as expressions, statements, and function definitions. This process not only verifies that the code adheres to the language's syntax but also creates a structured representation that subsequent compiler stages can easily traverse and manipulate for further analysis and code generation. The common way of expression grammar in programming language design is using [Backus-Naur Form (BNF)](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form), which is one of the well-known context-free grammar. 

Now you know what is context-free grammar, let's consider this grammar below:
```
program -> function
function -> "int" identifier "(" "void" ")" "{" statement "}"
statement -> "return" <exp> ";"
exp -> int
identifier -> TOKEN_IDENTIFIER
int -> TOKEN_INT
```

If you attempt to work through the return 0 code by using this parser, we got this AST:
```
Program(function_definition)
			| function_definition
			v
Function('main', body)
			| body
			v
		Return(exp)
			| exp
			v
		Constant(2)	
```

Before we go to parsing, let's outline AST definition first:
```c
struct Program {
	Function *function;
};

struct Function {
	const char *name;
	Statement *statement;
}; 

struct Statement {
	Expression *return_val;
};

struct Expression {
	int constant;
};
```
